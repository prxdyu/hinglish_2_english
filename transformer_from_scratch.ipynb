{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ccmhyribeniG"
      },
      "outputs": [],
      "source": [
        "# importing the required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import tensorflow_datasets as tfds\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt2272Lp2QIZ"
      },
      "source": [
        "### Importing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsYke_-J2xGe"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rlqmIxfRyE6N"
      },
      "outputs": [],
      "source": [
        "# importing train data\n",
        "train_data=pd.read_csv(\"data/TRAIN.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXU91Q01ySGE",
        "outputId": "5743c883-aed0-4146-bd1f-f9badf1fb8da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(315488, 2)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking the shape\n",
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSpB9nZ7yZWn",
        "outputId": "f4783256-448b-40e2-b455-3d50c1ac5c31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "english     0\n",
              "hinglish    5\n",
              "dtype: int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking the null values\n",
        "train_data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzY1AR5M2zXG"
      },
      "source": [
        "#### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J1mo5CFEyK6H"
      },
      "outputs": [],
      "source": [
        "# importing the validation data\n",
        "val_data=pd.read_csv(\"data/VALIDATION.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvgPt5MUyTl9",
        "outputId": "c120c199-6d30-4cf4-be5b-875e29f91374"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(41112, 2)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking the shape\n",
        "val_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UYwCOhbyd6o",
        "outputId": "dcd63493-8358-4cb2-d8bc-b34a266c346f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "english     0\n",
              "hinglish    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking for null values\n",
        "val_data.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoSsStFXyhtg"
      },
      "source": [
        "### Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8gYNcDEb3mse"
      },
      "outputs": [],
      "source": [
        "# defining a function that cleans the text\n",
        "def clean_text(s):\n",
        "  # changing the short form words like didn't, wouldn't\n",
        "  cleaned = re.sub(r\"n\\'t\", \" not\", s)\n",
        "  cleaned = re.sub(r\"\\'re\", \" are\", cleaned)\n",
        "  cleaned = re.sub(r\"\\'s\", \" is\", cleaned)\n",
        "  cleaned = re.sub(r\"\\'d\", \" would\", cleaned)\n",
        "  cleaned = re.sub(r\"\\'ll\", \" will\", cleaned)\n",
        "  cleaned = re.sub(r\"\\'t\", \" not\", cleaned)\n",
        "  cleaned = re.sub(r\"\\'ve\", \" have\", cleaned)\n",
        "  cleaned = re.sub(r\"\\'m\", \" am\", cleaned)\n",
        "  cleaned = re.sub(\"(\\s+)\", \" \", cleaned)\n",
        "  out = cleaned.translate(str.maketrans('', '', string.punctuation))\n",
        "  return out\n",
        "\n",
        "# defining a function to add special tokens\n",
        "def add_special_tokens(s):\n",
        "  return '<SOS>'+' '+s+' '+'<EOS>'\n",
        "\n",
        "# defining a function which preprocess the data\n",
        "def preprocess(df):\n",
        "\n",
        "  # remove the NaNs\n",
        "  df=df.dropna()\n",
        "  # removing the duplicates\n",
        "  df=df.drop_duplicates()\n",
        "  # applying the clean_text function to remove punctuations from hindi\n",
        "  df['hinglish']=df['hinglish'].apply(lambda x: clean_text(x))\n",
        "  # making all hinglish words into lower case\n",
        "  df['hinglish']=df['hinglish'].apply(lambda x: x.lower())\n",
        "\n",
        "  # applying the clean_text function to english\n",
        "  df['english']=df['english'].apply(lambda x: clean_text(x))\n",
        "  # making all english words into lower case\n",
        "  df['english']=df['english'].apply(lambda x: x.lower())\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lNHVsSmS4l-F"
      },
      "outputs": [],
      "source": [
        "# preprocessing train_data\n",
        "train_df = preprocess(train_data)\n",
        "\n",
        "# preprocessing the validation data\n",
        "val_df = preprocess(val_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ckSd37k-5oqr"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "XOU9bKGb5R8p",
        "outputId": "754dc6b1-4f21-4ced-9e85-1c1b52f394cd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'thodi dark comedy involved hai ek point me mean girls me se ek ko bus thok deti hai to family night se jyada date night k liye hai'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# testing\n",
        "ii=random.randint(0,1000)\n",
        "train_df.iloc[ii][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V2ezi_4Y9P9",
        "outputId": "b3f82ec5-3981-4f7b-8ffe-076798d160e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi\n",
            "tumne konsi movie dekhi\n",
            "hello tum kaise ho kya tumne batman begins ke bare mein suna hai kya great movie hai\n",
            "nahi aur batao\n"
          ]
        }
      ],
      "source": [
        "for j,i in enumerate(train_df.iterrows()):\n",
        "  print(i[1][1])\n",
        "  if j==3:\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXabZmFWnWX9"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE_-q7v-qn-l",
        "outputId": "abb9b593-fd52-40a0-d3c3-fab07f7e6fbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizers loaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Define file paths for saved tokenizers\n",
        "hindi_tokenizer_path = \"./hindi_tokenizer.subwords\"\n",
        "en_tokenizer_path = \"./en_tokenizer.subwords\"\n",
        "\n",
        "# Check if the saved tokenizers exist\n",
        "if os.path.exists(hindi_tokenizer_path+\".subwords\") and os.path.exists(en_tokenizer_path+\".subwords\"):\n",
        "    # Load the Hindi tokenizer\n",
        "    hindi_tokenizer = tfds.deprecated.text.SubwordTextEncoder.load_from_file(hindi_tokenizer_path)\n",
        "\n",
        "    # Load the English tokenizer\n",
        "    en_tokenizer = tfds.deprecated.text.SubwordTextEncoder.load_from_file(en_tokenizer_path)\n",
        "    print(\"Tokenizers loaded successfully.\")\n",
        "else:\n",
        "    print(\"Tokenizers not found. Building from scratch\")\n",
        "    SOS_TOKEN='<SOS>'\n",
        "    EOS_TOKEN='<EOS>'\n",
        "\n",
        "    # building the tokenizer for hinglish\n",
        "    hindi_tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "        corpus_generator=(row[1][1] for row in train_df.iterrows()),\n",
        "        target_vocab_size=2**13,\n",
        "        reserved_tokens=[SOS_TOKEN,EOS_TOKEN])\n",
        "\n",
        "    # building the tokenizer for english\n",
        "    en_tokenizer=tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
        "                corpus_generator=(row[1][0] for row in train_df.iterrows()),\n",
        "                target_vocab_size=2**13,\n",
        "                reserved_tokens=[SOS_TOKEN,EOS_TOKEN])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "knrZ68htpoGB"
      },
      "outputs": [],
      "source": [
        "# # saving the tokenizers\n",
        "\n",
        "# # Define file paths to save the tokenizers\n",
        "# hindi_tokenizer_path = \"hindi_tokenizer\"\n",
        "# en_tokenizer_path = \"en_tokenizer\"\n",
        "\n",
        "# # Save the Hindi tokenizer\n",
        "# hindi_tokenizer.save_to_file(hindi_tokenizer_path)\n",
        "\n",
        "# # Save the English tokenizer\n",
        "# en_tokenizer.save_to_file(en_tokenizer_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpVQS27yclEW",
        "outputId": "c4d6edd6-50b2-4d3b-e61d-5b3a4f34fb75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1-----> <SOS>\n",
            "7911----->  \n",
            "1189-----> hello \n",
            "620-----> tum \n",
            "367-----> kaise \n",
            "137-----> ho \n",
            "6-----> kya \n",
            "1510-----> tumne \n",
            "1607-----> batman \n",
            "7847-----> begins\n",
            "7911----->  \n",
            "3-----> ke \n",
            "318-----> bare \n",
            "139-----> mein \n",
            "2545-----> suna \n",
            "12-----> hai \n",
            "6-----> kya \n",
            "1042-----> great \n",
            "216-----> movie \n",
            "12-----> hai \n",
            "2-----> <EOS>\n"
          ]
        }
      ],
      "source": [
        "# testing the tokenization\n",
        "sample_string=\"<SOS> hello tum kaise ho kya tumne batman begins ke bare mein suna hai kya great movie hai <EOS>\"\n",
        "\n",
        "tokenized_string=hindi_tokenizer.encode(sample_string)\n",
        "og_string=hindi_tokenizer.decode(tokenized_string)\n",
        "\n",
        "for token in tokenized_string:\n",
        "  print(f\"{str(token)}-----> {hindi_tokenizer.decode([token])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eE518UVc_u1",
        "outputId": "7496c66f-bd2a-48f2-d2a4-252e4516ea09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hindi Vocab size 8135\n",
            "English Vocab size 8188\n"
          ]
        }
      ],
      "source": [
        "# finding the vocab sizes\n",
        "print(\"Hindi Vocab size\",hindi_tokenizer.vocab_size)\n",
        "print(\"English Vocab size\",en_tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oFQjzLhxdIHG"
      },
      "outputs": [],
      "source": [
        "# defining a function which adds CLS and SEP tokens in the beginning and in the end\n",
        "\n",
        "def encode(lang1,lang2):\n",
        "  \"\"\"\n",
        "  lang1: hindi\n",
        "  lang2: english\n",
        "  encodes the tokens to indices\n",
        "  \"\"\"\n",
        "  lang1= hindi_tokenizer.encode(lang1.numpy())\n",
        "  lang2= en_tokenizer.encode(lang2.numpy())\n",
        "  return lang1,lang2\n",
        "\n",
        "\n",
        "# wrapping our encode function using tf_encode function\n",
        "def tf_encode(hin,en):\n",
        "  hin_result,en_result=tf.py_function(encode,[hin,en],[tf.int64,tf.int64])\n",
        "  hin_result.set_shape([None])\n",
        "  en_result.set_shape([None])\n",
        "  return hin_result,en_result\n",
        "\n",
        "# writing a function to filter our data ie remove datapoints (hin,en) where if any one (pt or en) has more than 40 tokens\n",
        "def max_len_filter(hin,en,max_len=40):\n",
        "  # returning a mask (1 if both pt and en has less than 40 tokens each else 0)\n",
        "  return tf.logical_and(tf.size(hin)<=max_len,\n",
        "                        tf.size(en)<=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TccGtl-beLM3"
      },
      "outputs": [],
      "source": [
        "# converting pandas dataframe to tensorflow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_df['hinglish'].values, train_df['english'].values))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_df['hinglish'].values, val_df['english'].values))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "glIeaMrJdstZ"
      },
      "outputs": [],
      "source": [
        "# applying the tf_encode to our train dataset\n",
        "train_data=train_dataset.map(tf_encode)\n",
        "# filtering the train data\n",
        "train_data=train_data.filter(max_len_filter)\n",
        "\n",
        "# applying the tf_encode to our validation dataset\n",
        "val_data=val_dataset.map(tf_encode)\n",
        "# filtering the validation data\n",
        "val_data=val_data.filter(max_len_filter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "A5E1Y9-Yd47e"
      },
      "outputs": [],
      "source": [
        "# defining the global variables\n",
        "BATCH_SIZE=64\n",
        "BUFFER_SIZE=2000\n",
        "\n",
        "# caching the train data\n",
        "train_data=train_data.cache()\n",
        "#shuffling and padding the train data\n",
        "train_data=train_data.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
        "# prefetching batches in training data\n",
        "train_data=train_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# caching the val data\n",
        "val_data=val_data.cache()\n",
        "# shuffling and padding the val data\n",
        "val_data=val_data.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE)\n",
        "# prefetching batches in val data\n",
        "val_data=val_data.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55a_OhF0esDj",
        "outputId": "03c6c19d-cb4f-4d73-ee59-e87e86b0cd19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 319  129    7 ...    0    0    0]\n",
            " [ 649 3571  139 ...    0    0    0]\n",
            " [ 369  161   12 ...    0    0    0]\n",
            " ...\n",
            " [  50  111 1310 ...    0    0    0]\n",
            " [  27   23  567 ...    0    0    0]\n",
            " [   6 4391 7091 ...    0    0    0]], shape=(64, 38), dtype=int64) \n",
            "\n",
            "tf.Tensor(\n",
            "[[  13    6   20 ...    0    0    0]\n",
            " [  45    6 5939 ...    0    0    0]\n",
            " [ 213 1586  369 ...    0    0    0]\n",
            " ...\n",
            " [   7 1051   20 ...    0    0    0]\n",
            " [  25    8   11 ...    0    0    0]\n",
            " [  49   34 6913 ...    0    0    0]], shape=(64, 34), dtype=int64)\n"
          ]
        }
      ],
      "source": [
        "# printing a batch from the validation dataset\n",
        "\"\"\" a batch is of dimension (batch_size,maxlen)\"\"\"\n",
        "hin_batch,en_batch=next(iter(train_data))\n",
        "print(hin_batch,'\\n')\n",
        "print(en_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAZwgBrDBOiN"
      },
      "source": [
        "## Positional Encodings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmt_p-u1BGJ4"
      },
      "source": [
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CEcuUy92-Etj"
      },
      "outputs": [],
      "source": [
        "# definfing the function to create angle_matrix which contains angles for sin and cos\n",
        "def get_angles(pos,i,d_model):\n",
        "  \"\"\"\n",
        "  pos     : column vector (pos,1) having position values 0 to pos-1\n",
        "  i       : row vector (1,d_model) having values from 0 to d_models-1\n",
        "  d_model : embedding_dim\n",
        "\n",
        "  returns\n",
        "  angle_matrix: a matrix of dim (pos,d_model)\n",
        "\n",
        "  \"\"\"\n",
        "  angles= 1/ np.power (10000,(2*(i//np.float32(d_model))))\n",
        "  # assume pos=5 and d_model=512 then (5,1)*(1,512) => (5,512) dimensions of angles will get broadcasted to match the dim of pos\n",
        "  angle_matrix=pos*angles\n",
        "\n",
        "  return angle_matrix\n",
        "\n",
        "\n",
        "def positional_encodings(pos,d_model):\n",
        "\n",
        "  # creating a column vector (pos,1) from 0 to pos-1\n",
        "  pos=np.arange(pos)[:,np.newaxis]\n",
        "  # creating a row vector (1,d_model) from 0 to d_model-1\n",
        "  i=np.arange(d_model)[np.newaxis,:]\n",
        "  # passing the two vectors and d_model scalar to the get_angle fuction which returns a 2d matrix of dim (pos,d_model)\n",
        "  angle_matrix = get_angles(pos,i,d_model)\n",
        "\n",
        "  # applying sin function to even col indices in the matrix\n",
        "  angle_matrix[:,0::2]=np.sin(angle_matrix[:,0::2])\n",
        "\n",
        "  # applying the cos function to odd col indices in the matrix\n",
        "  angle_matrix[:,1::2]=np.cos(angle_matrix[:,1::2])\n",
        "\n",
        "  # after we apply sin and cos to the angle matrix it becomes positional encodings and we convert this matrix into a tensor\n",
        "  pos_encodings=angle_matrix[np.newaxis,...]\n",
        "  pos_encodings = tf.convert_to_tensor(pos_encodings, dtype=tf.float32)\n",
        "\n",
        "  return pos_encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTJDUsyWDszJ"
      },
      "source": [
        "### Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gscCsVJD-Jro"
      },
      "outputs": [],
      "source": [
        "# defining a function which creates padding mask ie it returns a binary vecotor where 1 represent corresponding token is a padding token ie \"0\"\n",
        "def create_padding_mask(seq):\n",
        "  mask=tf.cast(tf.math.equal(seq,0),tf.float32)\n",
        "  # making this as a tensor\n",
        "  mask=mask[:,tf.newaxis,tf.newaxis,:]\n",
        "  return mask\n",
        "\n",
        "# defining a function for look ahead mask\n",
        "def create_look_ahead_mask(size):\n",
        "  mask=1-tf.linalg.band_part(tf.ones((size,size)),-1,0)\n",
        "  return mask\n",
        "\n",
        "# defining functions which creates aboves two masks\n",
        "def create_masks(inp,target):\n",
        "  # creating encoder padding mask\n",
        "  enc_padding_mask= create_padding_mask(inp)\n",
        "  # creating decoder padding mask which will be used in the 2nd attention in the decoder layer\n",
        "  dec_padding_mask= create_padding_mask(inp)\n",
        "  # creating look_ahead_mask for masking future tokens in the decoder layer which will be used in the 1st attention in the decoder\n",
        "  look_ahead_mask= create_look_ahead_mask(tf.shape(target)[1])\n",
        "  # creating the padding mask for decoder\n",
        "  dec_target_padding_mask= create_padding_mask(target)\n",
        "\n",
        "  # creating a combined mask\n",
        "  combined_mask= tf.maximum(dec_target_padding_mask,look_ahead_mask)\n",
        "\n",
        "  return enc_padding_mask,combined_mask,dec_padding_mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jZwo1bpENsM"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjbpP7qlGXDv"
      },
      "source": [
        "### Scaled Dot Product Attention:\n",
        "$${Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "-_GMBbVWJiPt"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(k,v,q,mask):\n",
        "  \"\"\"\n",
        "  This is a self attention so q,k and v is build from the datamatrix\n",
        "  q: data tensor after passing through by linear layer Wq , (batch_size,n_heads,seq_len,depth)\n",
        "  k: data tensor after passing through by linear layer Wk , (batch_size,n_heads,seq_len,depth)\n",
        "  v: data tensor after passing through by linear layer Wv , (batch_size,n_heads,seq_len,depth)\n",
        "  \"\"\"\n",
        "\n",
        "  \"\"\"\n",
        "  k_transpose   : (batch_size,n_heads,depth_seq_len)\n",
        "  q.k_transpose : (batch_size,n_heads,seq_len,depth) * (batch_size,n_heads,depth,seq_len) ==> (batch_size,n_heads,seq_len,seq_len)\n",
        "  \"\"\"\n",
        "  # matrix multiplication of Q and K.transpose\n",
        "  matmul_qk=tf.matmul(q,k,transpose_b=True) # now this tensor has logits\n",
        "\n",
        "  # computing dk(embedding_dim) and casting it to float\n",
        "  dk=tf.cast(tf.shape(k)[-1],tf.float32)\n",
        "\n",
        "  # scaling the logits in the matmul_qk using dk\n",
        "  scaled_logits=matmul_qk/tf.math.sqrt(dk)\n",
        "\n",
        "  # since padding tokens contribute nothing to the attention we ignore them padding by adding a large negative numbers to the logits of the padding tokens\n",
        "  if mask is not None:\n",
        "    scaled_logits+=(mask * -1e9)\n",
        "\n",
        "  # applying the softmax function\n",
        "  attention_weights=tf.nn.softmax(scaled_logits,axis=-1)\n",
        "\n",
        "  # multiplying the attention weights with the v (batch_size,n_heads,seq_len,seq_len)*(batch_size,n_heads,seq_len,depth)\n",
        "  output=tf.matmul(attention_weights,v)\n",
        "\n",
        "  \"\"\"output            : (batch_size,n_heads,seq_len,depth)\n",
        "     attention_weights : (batch_size,n_heads,seq_len,seq_len) \"\"\"\n",
        "\n",
        "  return output,attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "L-3fAiGFc380"
      },
      "outputs": [],
      "source": [
        "# testing of function\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "temp_k = tf.constant([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype=tf.float32)  # (4, 2)\n",
        "\n",
        "# This `query` aligns with the second `key`,\n",
        "# so the second `value` is returned.\n",
        "temp_q = tf.constant([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype=tf.float32)  # (1, 3)\n",
        "# testing the function\n",
        "temp_out,temp_attn=scaled_dot_product_attention(temp_q,temp_k,temp_v,None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mMfSzJNEuWP"
      },
      "source": [
        "### Multi Head Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtGKJp71fTaN"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hf8H-LD0d_bD"
      },
      "outputs": [],
      "source": [
        "# implementing the Multi Head attention layer\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,d_model,n_heads):\n",
        "    \"\"\"\n",
        "    d_model: embedding_dim or no of hidden_units\n",
        "    n_heads: no of heads of self attention\n",
        "\n",
        "    \"\"\"\n",
        "    super(MultiHeadAttention,self).__init__()\n",
        "    self.n_heads=n_heads\n",
        "    self.d_model=d_model\n",
        "\n",
        "    # asserting if the d_model is divisible by the number of heads\n",
        "    assert d_model % self.n_heads == 0\n",
        "    # depth is the split of d_model for each head\n",
        "    self.depth = self.d_model // self.n_heads\n",
        "\n",
        "    # defining the linear layers for weight matrices Wk,Wq,Wv\n",
        "    self.wk=tf.keras.layers.Dense(d_model)\n",
        "    self.wq=tf.keras.layers.Dense(d_model)\n",
        "    self.wv=tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    # defining the linear layer for the last weight matrix W0\n",
        "    self.w0=tf.keras.layers.Dense(d_model)\n",
        "\n",
        "\n",
        "  def split_heads(self,x,batch_size):\n",
        "    \"\"\"\n",
        "    x: tensor of dim (batch_size,seq_len,embedding)\n",
        "    splits the tensors along last (embeddings) dim to pass that slice for each head\n",
        "    \"\"\"\n",
        "    # splitting the last dimension of x into (n_heads,depth)\n",
        "    x=tf.reshape(x,(batch_size,-1,self.n_heads,self.depth))\n",
        "\n",
        "    # after reshaping the shape of x is (batch_size,seq_len,n_heads,depth) but we want to change the dim order to (batch_size,n_heads,seq_len,depth) so permuting it\n",
        "    x=tf.transpose(x,perm=[0,2,1,3])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "  def call(self,k,v,q,mask):\n",
        "    \"\"\"\n",
        "    k: tensor of shape (batch_size,seq_len,embedding)\n",
        "    q: tensor of shape (batch_size,seq_len,embedding)\n",
        "    v: tensor of shape (batch_size,seq_len,embedding)\n",
        "    \"\"\"\n",
        "    # getting the batch size\n",
        "    batch_size=tf.shape(q)[0]\n",
        "\n",
        "    # passing the k,v,q to the linear layers wk,wv,wq respectively\n",
        "    k=self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v=self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    q=self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    # splitting the last dimension of k,v,q\n",
        "    k=self.split_heads(k,batch_size)  # (batch_size, n_heads, seq_len, depth)\n",
        "    v=self.split_heads(v,batch_size)  # (batch_size, n_heads, seq_len, depth)\n",
        "    q=self.split_heads(q,batch_size)  # (batch_size, n_heads, seq_len, depth)\n",
        "\n",
        "\n",
        "    # passing the k,v,q to the scaled attention function to compute attention weights\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(k,v,q,mask)\n",
        "    \"\"\"\n",
        "    shape of scaled_attention : (batch_size,n_heads,seq_len,depth)\n",
        "    shape of attention_weights: (batch_size,n_heads,seq_len,seq_len)\n",
        "    \"\"\"\n",
        "    # transposing the scaled attention to make it prepare for reshaping\n",
        "    scaled_attention= tf.transpose(scaled_attention,perm=[0,2,1,3]) # (batch_size, seq_len, n_heads, depth)\n",
        "\n",
        "    # reshaping the scaled_attention back to the (batch_size,seq_len,d_model) d_model=n_heads*depth\n",
        "    concat_attention= tf.reshape(scaled_attention, (batch_size,-1,self.d_model))\n",
        "\n",
        "    # passing the concat_attention to the final linear layer which consists of w0\n",
        "    output=self.w0(concat_attention) # (batch_size, seq_len, d_model)\n",
        "\n",
        "    return output,attention_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKGy6rGCCD3e",
        "outputId": "a6245842-08d3-47a6-be31-1d07bed79038"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# testing the MultiHeadAttention\n",
        "temp_mha = MultiHeadAttention(d_model=512, n_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y,y,y, mask=None)\n",
        "out.shape, attn.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttepsCtqxQew"
      },
      "source": [
        "### Feed forward network\n",
        "After self attention we have layer norm followed by a feed forward network for each token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "InMn38khAydP"
      },
      "outputs": [],
      "source": [
        "def point_wise_fc_layer(d_model,dff):\n",
        "  \"\"\"\n",
        "  d_model: dimensionality of the input\n",
        "  dff    : hidden_units in the dense_layer\n",
        "\n",
        "  it returns a model consisting 2 FC layers. 1st layers reduces the dimensionality of input to dff, the 2nd layer converts the dimensionality of input back to d_model\"\"\"\n",
        "  return tf.keras.Sequential([tf.keras.layers.Dense(dff,activation='relu'), # (batch_size,seq_len,dff)\n",
        "                              tf.keras.layers.Dense(d_model)])              # (batch_size,seq_len,d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVZoievGB8jf",
        "outputId": "fbafb52a-4ab1-4635-d3c5-f543d447a6c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# testing of point_wise_fc_layer\n",
        "sample_ffn = point_wise_fc_layer(512, 2048)\n",
        "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO6TE96IDiII"
      },
      "source": [
        "### Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "GeupuRNfETsx"
      },
      "outputs": [],
      "source": [
        "# implementing Single Encoder layer\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,d_model,num_heads,dff,rate=0.1):\n",
        "    super(EncoderLayer,self).__init__()\n",
        "\n",
        "    # defining the multi head attention layer\n",
        "    self.mha=MultiHeadAttention(d_model,num_heads)\n",
        "    # defining the point wise feed forward layer\n",
        "    self.fc=point_wise_fc_layer(d_model,dff)\n",
        "\n",
        "    # defining the layer norm 1 and 2\n",
        "    self.layer_norm1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layer_norm2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    # defining the dropouts\n",
        "    self.dropout1=tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2=tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "  def call(self,x,training,mask):\n",
        "\n",
        "    # since it is a self attention we pass the same input x as key,query,value to the multi-head-attention\n",
        "    attn_output,_= self.mha(k=x,v=x,q=x,mask=mask) # (batch_size,seq_len,d_model)\n",
        "    # passing the attn_output through the dropout\n",
        "    attn_output= self.dropout1(attn_output,training=training)\n",
        "\n",
        "    # passing the output throught the layer norm\n",
        "    out1= self.layer_norm1(x+attn_output)\n",
        "\n",
        "    # passing the output from the layer norm to the FC layer\n",
        "    fc_output=self.fc(out1)\n",
        "    # passing throught the dropout\n",
        "    fc_output=self.dropout2(fc_output)\n",
        "\n",
        "    # passing the ouput to the layer norm2\n",
        "    out2=self.layer_norm2(fc_output) # (batch_size, seq_len, d_model)\n",
        "\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEydxj0EIUzc",
        "outputId": "ba1c8dd1-bee6-42d8-fe88-359d199a0b05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# testing of the encoder layer class\n",
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQyTFqEpGLSp"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "wrctkGiAHsbs"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,num_layers,d_model,num_heads,dff,input_vocab_size,max_pos_encoding,rate=0.1):\n",
        "    \"\"\"\n",
        "    num_layers        : no of encoder layers\n",
        "    d_model           : dimensionality of input embeddings\n",
        "    num_heads         : no of heads in multi-head attention\n",
        "    input_vocab_size  : no of words in input language vocab\n",
        "    max_pos_encoding  :\n",
        "    dff               :\n",
        "    max_pos_encoding  :\n",
        "    \"\"\"\n",
        "    super(Encoder,self).__init__()\n",
        "\n",
        "    self.d_model=d_model\n",
        "    self.num_heads=num_heads\n",
        "    self.num_layers=num_layers\n",
        "\n",
        "    # defining the embedding layer\n",
        "    self.embeddings=tf.keras.layers.Embedding(input_vocab_size,d_model)\n",
        "    # defining the positional encoding layer\n",
        "    self.pos_encodings=positional_encodings(max_pos_encoding,self.d_model)\n",
        "\n",
        "    # defining the encoding layers\n",
        "    self.enc_layers=[EncoderLayer(d_model,num_heads,dff,rate) for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout=tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "\n",
        "  def call(self,x,training,mask):\n",
        "\n",
        "    \"\"\"\n",
        "    x        : input tensor of shape (batch_size, seq_len)\n",
        "    training : bool variable to indicate whether we are in training\n",
        "    mask     : masks for padding\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # getitng the seq_len of the inputs\n",
        "    seq_len=tf.cast(tf.shape(x)[1], tf.int32)\n",
        "\n",
        "    # passing the input throught the emebdding layer\n",
        "    x=self.embeddings(x)  # x:(batch_size, seq_len, d_model)\n",
        "\n",
        "    # normalization\n",
        "    x*=tf.math.sqrt(tf.cast(self.d_model,tf.float32))\n",
        "\n",
        "    # passing the embedding to the positional encodng\n",
        "    x+=self.pos_encodings[:,:seq_len,:]\n",
        "\n",
        "    # adding dropout\n",
        "    x=self.dropout(x,training=training)\n",
        "\n",
        "    # adding encoder layers\n",
        "    for i in range(self.num_layers):\n",
        "      x=self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha-qhEP1OT9U",
        "outputId": "ebfc459e-8008-4861-ea64-0fdea21ee300"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 62, 512)\n"
          ]
        }
      ],
      "source": [
        "  # testing of Encoder\n",
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8,\n",
        "                         dff=2048, input_vocab_size=8500,\n",
        "                         max_pos_encoding=10000)\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
        "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIat2JWRIWfd"
      },
      "source": [
        "### Decoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WRUzD4ikO5JJ"
      },
      "outputs": [],
      "source": [
        "# implementing Single Decoder layer\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,d_model,num_heads,dff,rate=0.1):\n",
        "    super(DecoderLayer,self).__init__()\n",
        "\n",
        "    # defining the multi head attention layer1\n",
        "    self.mha1=MultiHeadAttention(d_model,num_heads)\n",
        "    # definfing the multi head attention layer2 (encoder-decoder attention)\n",
        "    self.mha2=MultiHeadAttention(d_model,num_heads)\n",
        "\n",
        "    # defining the point wise feed forward layer\n",
        "    self.fc=point_wise_fc_layer(d_model,dff)\n",
        "\n",
        "    # defining the layer norm 1,2 and 3\n",
        "    self.layer_norm1=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layer_norm2=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layer_norm3=tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "\n",
        "    # defining the dropouts\n",
        "    self.dropout1=tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2=tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3=tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "\n",
        "  def call(self,x,enc_output,training,look_ahead_mask,padding_mask):\n",
        "    \"\"\"\n",
        "    x          : input at the current timestep t\n",
        "    enc_output : output of the encoder with shape (batch_size, seq_len, d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    # passing the target as the inputs to the multi-head-attention\n",
        "    attn1,attn_weights1,=self.mha1(x,x,x,look_ahead_mask)\n",
        "    # passing throught dropouts\n",
        "    attn1=self.dropout1(attn1,training=training)\n",
        "    # passing through layer norm\n",
        "    out1=self.layer_norm1(attn1 + x)\n",
        "\n",
        "    # passing the encoders output as v and k and passing out1 as q to the multi-head-attention(encoder_decoder_attention)\n",
        "    attn2,attn_weights2=self.mha2(enc_output,enc_output,out1,padding_mask)\n",
        "    # passing through dropout\n",
        "    attn2=self.dropout2(attn2,training=training)\n",
        "    # passing through selfnorm\n",
        "    out2=self.layer_norm2(attn2 + out1)\n",
        "\n",
        "    # passing through the fc layer\n",
        "    fc_output=self.fc(out2)\n",
        "    # passing through dropout\n",
        "    fc_output=self.dropout3(fc_output)\n",
        "    # passing through layer norm\n",
        "    out3=self.layer_norm3(fc_output+out2)\n",
        "\n",
        "    return out3,attn_weights1,attn_weights2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfCEoUOW70I9",
        "outputId": "e45a15ae-ce6e-4572-96ec-724d2183d8eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# testing of decoder layer\n",
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output,\n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqe3zIo_OW1e"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "XS5-H1S2P3sj"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,num_layers,d_model,num_heads,dff,target_vocab_size,max_pos_encoding,rate=0.1):\n",
        "    \"\"\"\n",
        "    num_layers        : number of decoder layers\n",
        "    d_model           : dimensionality of target embedding\n",
        "    num_heads         : number of heads in decoder multi-head attention\n",
        "    dff               : hidden units in the decoder fc layer\n",
        "    target_vocab_size : vocab size of target\n",
        "    \"\"\"\n",
        "\n",
        "    super(Decoder,self).__init__()\n",
        "\n",
        "    self.d_model=d_model\n",
        "    self.num_layers=num_layers\n",
        "\n",
        "    # defining the embedding layer in the decoder\n",
        "    self.embeddings=tf.keras.layers.Embedding(target_vocab_size,d_model)\n",
        "\n",
        "    # defining the positional encoding layer\n",
        "    self.pos_encodings=positional_encodings(max_pos_encoding,self.d_model)\n",
        "\n",
        "    # defining the encoding layers\n",
        "    self.dec_layers=[DecoderLayer(d_model,num_heads,dff,rate) for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout=tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "\n",
        "  def call(self,x,enc_output,training,look_ahead_mask,padding_mask):\n",
        "    \"\"\"\n",
        "    x          : input tensor of target words with shape (batch_size,seq_len)\n",
        "    enc_output : output from the encoding layer with shape (batch_size, seq_len, d_model)\n",
        "    training   : boolean variable to indicate whether we are training\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # getting the seq_len of the target inputs\n",
        "    seq_len=tf.cast(tf.shape(x)[1],tf.float32)\n",
        "\n",
        "    attn_weights={}\n",
        "\n",
        "    # passing the input target words to the embedding layer\n",
        "    x=self.embeddings(x)  # x: (batch_size, seq_len, d_model)\n",
        "\n",
        "    # normalization\n",
        "    x*=tf.math.sqrt(tf.cast(self.d_model,tf.float32))\n",
        "\n",
        "    # positional encodings\n",
        "    x += self.pos_encodings[:, :tf.cast(seq_len, tf.int32), :]\n",
        "\n",
        "\n",
        "    # adding dropouts\n",
        "    x=self.dropout(x,training=training)\n",
        "\n",
        "    # passing through the decoder layers\n",
        "    for i in range(self.num_layers):\n",
        "      x,block1,block2=self.dec_layers[i](x,enc_output,training,look_ahead_mask,padding_mask)\n",
        "      attn_weights[f'decoder_layer{i+1}_block1']=block1\n",
        "      attn_weights[f'decoder_layer{i+1}_block2']=block2\n",
        "\n",
        "    return x,attn_weights # x: (batch_size,seq_len,d_model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jl74XWSmUQAk",
        "outputId": "184cb041-9eb1-45f5-80af-4587b1a14009"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TESING OF DECODER\n",
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8,\n",
        "                         dff=2048, target_vocab_size=8000,\n",
        "                         max_pos_encoding=5000)\n",
        "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
        "output, attn = sample_decoder(temp_input,\n",
        "                              enc_output=sample_encoder_output,\n",
        "                              training=False,\n",
        "                              look_ahead_mask=None,\n",
        "                              padding_mask=None)\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mdg_BKAiUS8t"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "8QzXz0irVDhw"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "\n",
        "  def __init__(self,num_layers,d_model,num_heads,dff,input_vocab_size,target_vocab_size,pe_input,pe_target,rate=0.1):\n",
        "    \"\"\"\n",
        "    num_layers        : number of encoder/decoder layers\n",
        "    d_model           : dimensionality of target embedding\n",
        "    num_heads         : number of heads in decoder multi-head attention\n",
        "    dff               : hidden units in the decoder fc layer\n",
        "    input_vocab_size  : no of words in input language vocab\n",
        "    target_vocab_size : vocab size of target\n",
        "    pe_iput           : positional encodings of input embeddings\n",
        "    pe_output         : positional encodings of target embeddings\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    super(Transformer,self).__init__()\n",
        "\n",
        "    # defining the encoder\n",
        "    self.encoder= Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
        "\n",
        "    # defining the decoder\n",
        "    self.decoder= Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "    # defining the final classfier layer\n",
        "    self.final_layer=tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  def call(self,inp,target,training,enc_padding_mask,look_ahead_mask,dec_padding_mask):\n",
        "    \"\"\"\n",
        "    inp    : input tesor of shape (batch_size, input_seq_len)\n",
        "    target : target tensor of shape (batch_size, target_seq_len)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # passing the input to the encoder\n",
        "    enc_output= self.encoder(inp,training,enc_padding_mask)  #(batch_size,input_seq_len,d_model)\n",
        "\n",
        "    # passing the target to the decoder\n",
        "    dec_output,attention_weights=self.decoder(target,enc_output,training,look_ahead_mask,dec_padding_mask)\n",
        "\n",
        "    # passing the decoder output to the final layer\n",
        "    final_output=self.final_layer(dec_output)  # (batch_size,target_seq_len,target_vocab_size)\n",
        "\n",
        "    return final_output,attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfpqta73Xv5s",
        "outputId": "4832fa77-6e62-443e-99da-13bd9cc2af62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TensorShape([64, 36, 8000])"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TESTING OF TRANSFORMER\n",
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048,\n",
        "    input_vocab_size=8500, target_vocab_size=8000,\n",
        "    pe_input=10000, pe_target=6000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_input, temp_target, training=False,\n",
        "                               enc_padding_mask=None,\n",
        "                               look_ahead_mask=None,\n",
        "                               dec_padding_mask=None)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFmNjAGQX10d"
      },
      "source": [
        "#### Custom Scheduler\n",
        "\n",
        "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "EMOQJCQyifah"
      },
      "outputs": [],
      "source": [
        "# creating a custom scheduler which changes the learning rate according the formula in the research paper\n",
        "class CustomScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "  def __init__(self,d_model,warmup_steps=4000):\n",
        "    self.d_model=d_model\n",
        "    self.d_model=tf.cast(self.d_model,tf.float32)\n",
        "    self.warmup_steps=warmup_steps\n",
        "\n",
        "\n",
        "  def __call__(self,step):\n",
        "    step = tf.cast(step, tf.float32)\n",
        "    arg1= tf.math.rsqrt(step)\n",
        "    arg2= step * (self.warmup_steps ** -1.5)\n",
        "    return tf.math.rsqrt(self.d_model)*tf.math.minimum(arg1,arg2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLu2rXRVjqjX"
      },
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "XmE1V-oRmgcH"
      },
      "outputs": [],
      "source": [
        "# creating the loss object\n",
        "loss_obj=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')\n",
        "\n",
        "# creating a loss function\n",
        "def loss_function(real,pred):\n",
        "\n",
        "  # creating a mask because we need to ignore padding tokens while calculating the loss\n",
        "  mask=tf.math.logical_not(tf.math.equal(real,0)) # this is a tensor which contains 0 if the corresponding token is a padding token ie 0 else 1\n",
        "\n",
        "  # creating the loss obj\n",
        "  loss_=loss_obj(real,pred)\n",
        "\n",
        "  # casting the mask\n",
        "  mask=tf.cast(mask,dtype=loss_.dtype)\n",
        "\n",
        "  # we need to multiply the loss with the mask to ignore the losses corresponding to the token\n",
        "  loss_*=mask\n",
        "\n",
        "  # calculating the avg loss\n",
        "  total_loss= tf.reduce_sum(loss_) # total loss for the non padding tokens\n",
        "  non_padding_tokens= tf.reduce_sum(mask) # total number of non padding tokens\n",
        "  avg_loss= total_loss/non_padding_tokens\n",
        "\n",
        "  return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf_ZhYnCrmw1"
      },
      "source": [
        "### Accuracy function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "JEUKhNHwtcU8"
      },
      "outputs": [],
      "source": [
        "def accuracy_func(real, pred):\n",
        "    \"\"\"\n",
        "    real : ground truth matrix of shape (batch_size, seq_len)\n",
        "    pred : predicted tensor of shape (batch_size, seq_len, target_voc_size)\n",
        "    \"\"\"\n",
        "\n",
        "    # computing the correctly predicted words\n",
        "    correctly_predicted = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "    # creating a mask for padding tokens in the real\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "\n",
        "    # calculating the correctly predicted words excluding the padding tokens\n",
        "    accuracies = tf.math.logical_and(correctly_predicted, mask)\n",
        "\n",
        "    # calculating the accuracy\n",
        "    accuracy = tf.reduce_sum(tf.cast(accuracies, dtype=tf.float32)) / tf.reduce_sum(tf.cast(mask, dtype=tf.float32))\n",
        "\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygitdPccvxNE"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "Uzxgh-nCwDWw"
      },
      "outputs": [],
      "source": [
        "# defining the hyper parameters\n",
        "NUM_LAYERS = 4\n",
        "D_MODEL = 128\n",
        "DFF = 512\n",
        "NUM_HEADS = 8\n",
        "DROPOUT_RATE = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "oFSVPNKJv2BT"
      },
      "outputs": [],
      "source": [
        "# defining a transformer model\n",
        "transformer= Transformer(num_layers=NUM_LAYERS,\n",
        "                         d_model=D_MODEL,\n",
        "                         num_heads=NUM_HEADS,\n",
        "                         dff=DFF,\n",
        "                         input_vocab_size=hindi_tokenizer.vocab_size,\n",
        "                         target_vocab_size=en_tokenizer.vocab_size,\n",
        "                         pe_input=1000,\n",
        "                         pe_target=1000,\n",
        "                         rate=DROPOUT_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "2osT8yHczvLs"
      },
      "outputs": [],
      "source": [
        "# creating learning rate custom scheduler\n",
        "learning_rate= CustomScheduler(D_MODEL)\n",
        "# creating the optimizer\n",
        "optimizer= tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD2spJh_w7yR",
        "outputId": "3cca0267-3836-486d-e864-c83d4c7b80bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored!!\n"
          ]
        }
      ],
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCPpQxBA8agq"
      },
      "source": [
        "#### Train Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "aTJ06rPHze74"
      },
      "outputs": [],
      "source": [
        "# creating metrics for train\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "# wrapping the training function using tf.function wrapper to compile the steps into a TF Graph for faster execution\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp,target):\n",
        "  \"\"\"\n",
        "  inp    : input tensor of shape (batch_size, inp_seq_len)\n",
        "  target : target tensor of shape (batch_size, target_seq_len)\n",
        "\n",
        "  \"\"\"\n",
        "  # slicing the target by excluding the last token of all sequences in the batch to pass it to the decoder as input\n",
        "  tar_inp= target[:,:-1]\n",
        "  # slicing the target by excluding the first token of all sequences in the batch to pass it as the label for the decoder to compute loss\n",
        "  tar_real= target[:,1:]\n",
        "\n",
        "  # creating masks\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "  # Gradient Calculation\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # making predictions\n",
        "    predictions, _ = transformer(inp,\n",
        "                                 tar_inp,\n",
        "                                 True,\n",
        "                                 enc_padding_mask,\n",
        "                                 combined_mask,\n",
        "                                 dec_padding_mask)\n",
        "    # computing the loss\n",
        "    loss = loss_function(tar_real,predictions)\n",
        "\n",
        "\n",
        "    # computing the gradients\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "\n",
        "    # updating the params by applying the gradients\n",
        "    optimizer.apply_gradients(zip(gradients,transformer.trainable_variables))\n",
        "\n",
        "    # making the loss and accuracies as the metrics\n",
        "    train_loss(loss)\n",
        "    train_accuracy(accuracy_func(tar_real,predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad016ZWtmpVL"
      },
      "source": [
        "#### Val step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "fLWeNDrpjybO"
      },
      "outputs": [],
      "source": [
        "# creating metrics for validation\n",
        "val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
        "val_accuracy = tf.keras.metrics.Mean(name='val_accuracy')\n",
        "\n",
        "\n",
        "val_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "\n",
        "# wrapping the val_step function using tf.function wrapper to compile the steps into a TF Graph for faster execution\n",
        "@tf.function(input_signature=val_step_signature)\n",
        "def val_step(inp,target):\n",
        "  \"\"\"\n",
        "  inp    : input tensor of shape (batch_size, inp_seq_len)\n",
        "  target : target tensor of shape (batch_size, target_seq_len)\n",
        "\n",
        "  \"\"\"\n",
        "  # slicing the target by excluding the last token of all sequences in the batch to pass it to the decoder as input\n",
        "  tar_inp= target[:,:-1]\n",
        "  # slicing the target by excluding the first token of all sequences in the batch to pass it as the label for the decoder to compute loss\n",
        "  tar_real= target[:,1:]\n",
        "\n",
        "  # creating masks\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "\n",
        "  # making predictions\n",
        "  predictions, _ = transformer(inp,\n",
        "                                 tar_inp,\n",
        "                                 False,\n",
        "                                 enc_padding_mask,\n",
        "                                 combined_mask,\n",
        "                                 dec_padding_mask)\n",
        "  # computing the loss\n",
        "  loss = loss_function(tar_real,predictions)\n",
        "\n",
        "  # computing the accuracy\n",
        "  accuracy = accuracy_func(tar_real, predictions)\n",
        "\n",
        "  # making the loss and accuracies as the metrics\n",
        "  val_loss(loss)\n",
        "  val_accuracy(accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv0nNHUl7a_C"
      },
      "source": [
        "#### Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKXqVhfd8yF3"
      },
      "outputs": [],
      "source": [
        "# TRAINING\n",
        "num_epochs=40\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  # noting the time of the start\n",
        "  start=time.time()\n",
        "\n",
        "  # resetting the train loss and train accuracy to calculate fresh for the each epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # resetting the validation loss and val accuracy to calculate fresh for each epoch\n",
        "  val_loss.reset_states()\n",
        "  val_accuracy.reset_states()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # TRAINING\n",
        "  for (batch,(inp,tar)) in enumerate(train_data):\n",
        "    # training the batch using train_step function\n",
        "    train_step(inp,tar)\n",
        "\n",
        "    if batch % 250 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  # VALIDATION\n",
        "  for (batch,(inp, tar)) in enumerate(val_data):\n",
        "    # validating the batch using val_step fuction\n",
        "    val_step(inp, tar)\n",
        "\n",
        "\n",
        "\n",
        "  # Print training and validation metrics\n",
        "  print(f'\\n Epoch {epoch + 1} \\n Training Loss: {train_loss.result():.4f} Training Accuracy: {train_accuracy.result():.4f} \\n Validation Loss: {val_loss.result():.4f} Validation Accuracy: {val_accuracy.result():.4f}')\n",
        "  print(f'Time taken for epoch {epoch + 1}: {time.time() - start:.2f} secs\\n')\n",
        "\n",
        "  # Save checkpoint\n",
        "  ckpt_save_path = ckpt_manager.save()\n",
        "  print(f'Saving checkpoint for epoch {epoch + 1} at {ckpt_save_path}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBiOUbgSeSwF"
      },
      "source": [
        "### Model Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "M90DU9rCz33z"
      },
      "outputs": [],
      "source": [
        "# getting the index for the <EOS> token\n",
        "EOS_INDEX=en_tokenizer.encode('<EOS>')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "q-U7kYa3eWFl"
      },
      "outputs": [],
      "source": [
        "# defining a function to evaluate query at runtime\n",
        "def translate(sentence,maxlen=10):\n",
        "\n",
        "  \"\"\"sentence : hinglish sentence\"\"\"\n",
        "\n",
        "  # adding <SOS> and <EOS> tokens to the sentence\n",
        "  sentence= add_special_tokens(sentence)\n",
        "  target=\"<SOS>\"\n",
        "\n",
        "\n",
        "  # tokenization (converting the sequence to indices)\n",
        "  sentence=hindi_tokenizer.encode(sentence)\n",
        "  target=en_tokenizer.encode(target)\n",
        "\n",
        "  # converting the sentence to tf tensor\n",
        "  sentence=tf.convert_to_tensor(sentence)      # (,inp_len)\n",
        "\n",
        "  # adding a new dim to the sentence tensor\n",
        "  sentence = tf.expand_dims(sentence, axis=0)\n",
        "  encoder_input=sentence                       # (1, inp_len)\n",
        "\n",
        "  # converting the target to a tensor\n",
        "  target= tf.convert_to_tensor(target)         # (1, )\n",
        "  # adding a new dimension to the target tensor\n",
        "  target = tf.expand_dims(target, axis=0)      # (1, 1)\n",
        "\n",
        "  \"\"\"\n",
        "  Initially @ 1st timestep we pass <EOS> token to the transformer,\n",
        "  it generates maxlen ie 40 words, but we take the last word as the output of the 1st timestep\n",
        "  now we concatenate this ouput to the <EOS> and pass it to the transformer as the input @ 2nd timestep and\n",
        "  and again it generates 40 words we take the last word and concatenate it and pass it as the input @ 3rd timestep\n",
        "  \"\"\"\n",
        "\n",
        "  for i in range(maxlen):\n",
        "\n",
        "    # creating masks\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, target)\n",
        "\n",
        "    #enccoder_input : (1, inp_len)\n",
        "    # target        : (1,1)\n",
        "\n",
        "    # making predictions of shape (batch_size,target_seq_len,target_vocab_size)\n",
        "    predictions,attn_weights=transformer(encoder_input,\n",
        "                                         target,\n",
        "                                         False,\n",
        "                                         enc_padding_mask,\n",
        "                                         combined_mask,\n",
        "                                         dec_padding_mask)\n",
        "\n",
        "\n",
        "\n",
        "    # selecting the last word from predictions @ curr timestep\n",
        "    last_word = predictions[:,-1,:] # (batch_size,1,target_vocab_size)\n",
        "\n",
        "    # computing the index of the highest probability and last_word is a eager tensor containing the index\n",
        "    last_word = tf.argmax(last_word,axis=-1)\n",
        "\n",
        "    # converting the eager tensor to a int for if condition\n",
        "    last_word_idx = last_word.numpy().item()\n",
        "\n",
        "\n",
        "    # casting the last_word to int32\n",
        "    last_word_id  = last_word\n",
        "    last_word_id = tf.cast(last_word_id,tf.int32)\n",
        "    # reshaping last_word_id to match the dimension of target to concat both of them\n",
        "    last_word_id = tf.reshape(last_word_id, [1, 1])\n",
        "\n",
        "\n",
        "    # adding the last predicted word(index) @ curr timestep to the input for the transformer @ next timestep\n",
        "    target= tf.concat([target,last_word_id],axis=-1)\n",
        "\n",
        "    # if the last predicted word is <EOS> break the loop and stop generating\n",
        "    if last_word_idx==EOS_INDEX:\n",
        "      break\n",
        "\n",
        "  # decoding the indices in targets to texts\n",
        "  target=list(target.numpy()[0])\n",
        "  decoded=en_tokenizer.decode(target)\n",
        "\n",
        "  # removing the special tokens\n",
        "  decoded_lst= decoded.split(\" \")\n",
        "  decoded=[i for i in decoded_lst if i not in ('<SOS>','<EOS>') ]\n",
        "  decoded=\" \".join(decoded)\n",
        "\n",
        "  return decoded\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA54UGi-BEBE"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsg6ap-dK1cH",
        "outputId": "bbd01536-3c7e-4a4a-ad1c-4f0eeee68288"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "haan interesting tha jab stars sab apne areas par cross overs karthe new one ke saath\n",
            "yeah it is interesting when stars in other areas cross over to new ones right\n"
          ]
        }
      ],
      "source": [
        "sample_sentence=input(\"Enter a hinglish sentence: \")\n",
        "translated_sentence=translate(sample_sentence)\n",
        "print(sample_sentence)\n",
        "print(translated_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG8rCIkTBB4j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
